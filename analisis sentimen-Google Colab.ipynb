{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgqr2F3GD17ulvuxrwR4cA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"th0E1Q6-dsrQ"},"outputs":[],"source":["#scrap yt\n","# STEP 1: Install library yang dibutuhkan\n","!pip install google-api-python-client youtube-transcript-api pandas openai-whisper yt-dlp openpyxl --quiet\n","\n","# STEP 2: Import library\n","import pandas as pd\n","import os\n","import time\n","from googleapiclient.discovery import build\n","from youtube_transcript_api import YouTubeTranscriptApi\n","import whisper\n","\n","# STEP 3: API KEY YouTube\n","API_KEY = 'insert your api key'\n","\n","# STEP 4: Fungsi pencarian video YouTube\n","def search_youtube_videos(keyword, max_results=5):\n","    youtube = build('youtube', 'v3', developerKey=API_KEY)\n","    videos = []\n","    next_page_token = None\n","\n","    while len(videos) < max_results:\n","        request = youtube.search().list(\n","            q=keyword,\n","            part='snippet',\n","            type='video',\n","            maxResults=min(50, max_results - len(videos)),\n","            relevanceLanguage='id',\n","            videoCaption='any',\n","            pageToken=next_page_token\n","        )\n","        response = request.execute()\n","\n","        for item in response['items']:\n","            if 'videoId' in item['id']:\n","                video_id = item['id']['videoId']\n","                videos.append({\n","                    'video_id': video_id,\n","                    'title': item['snippet']['title'],\n","                    'link': f\"https://www.youtube.com/watch?v={video_id}\",\n","                    'channel_name': item['snippet']['channelTitle'],\n","                    'publish_date': item['snippet']['publishedAt']\n","                })\n","\n","        next_page_token = response.get('nextPageToken')\n","        if not next_page_token:\n","            break\n","\n","    return videos\n","\n","# STEP 5: Fungsi ambil transkrip dari YouTube/Whisper\n","def get_video_transcript(video_id):\n","    try:\n","        # Ambil subtitle otomatis dari YouTube\n","        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['id'])\n","        return \" \".join([t['text'] for t in transcript])\n","    except Exception as e:\n","        print(f\"Transkrip YouTube tidak tersedia: {e}\")\n","        print(\"Menggunakan Whisper...\")\n","\n","        try:\n","            # Download audio\n","            audio_file = f\"{video_id}.mp3\"\n","            os.system(f'yt-dlp --quiet --extract-audio --audio-format mp3 -o \"{video_id}.%(ext)s\" https://www.youtube.com/watch?v={video_id}')\n","\n","            # Transkripsi menggunakan Whisper\n","            if os.path.exists(audio_file):\n","                model = whisper.load_model(\"base\")\n","                result = model.transcribe(audio_file)\n","                os.remove(audio_file)  # Hapus file audio setelah transkripsi\n","                return result['text']\n","            else:\n","                return \"Audio file not found for Whisper.\"\n","        except Exception as whisper_error:\n","            return f\"Whisper failed: {whisper_error}\"\n","\n","# STEP 6: Fungsi utama\n","def main(keyword):\n","    print(f\"Mencari video dengan keyword: {keyword}\")\n","    videos = search_youtube_videos(keyword)\n","    results = []\n","\n","    for i, video in enumerate(videos, 1):\n","        print(f\"\\n[{i}] Judul: {video['title']}\")\n","        text = get_video_transcript(video['video_id'])\n","\n","        results.append({\n","            'No': i,\n","            'Link': video['link'],\n","            'Title': video['title'],\n","            'Text': text,\n","            'Channel': video['channel_name'],\n","            'Upload Date': video['publish_date']\n","        })\n","\n","        time.sleep(5)  # Delay untuk menghindari error 429 (rate limit)\n","\n","    df = pd.DataFrame(results)\n","    filename = 'youtube_results.xlsx'\n","    df.to_excel(filename, index=False)\n","    print(f\"\\nâœ… Data tersimpan ke: {filename}\")\n","\n","    # Download di Colab\n","    from google.colab import files\n","    files.download(filename)\n","\n","    return df\n","\n","# STEP 7: Jalankan pencarian\n","keyword = \"ikn ibu kota nusantara\"\n","df = main(keyword)\n","df.head()\n"]},{"cell_type":"code","source":[" #scrap Google News\n","import urllib.parse\n","import requests\n","import pandas as pd\n","from google.colab import files\n","\n","# 1. Definisikan kata kunci yang ingin Anda cari\n","keywords = ['ikn']\n","\n","for string in keywords:\n","    # 2. Inisialisasi list untuk menyimpan hasil\n","    title = []\n","    snippet = []\n","    url = []\n","    source = []\n","    date = []\n","\n","    # 3. Sesuaikan rentang halaman yang ingin Anda scraping\n","    for i in [0, 100]:\n","        # 4. Parameter pencarian\n","        params = {\n","            'q': string,\n","            'tbm': \"nws\",\n","            'location': \"Indonesia\",\n","            'api_key': \"insert your api key\",\n","            \"start\" : i,\n","            'num': \"100\",\n","            'lr': \"lang_id\",  # 5. Menambahkan filter untuk bahasa Indonesia\n","            'tbs': \"cdr:1,cd_min:1/1/2023,cd_max:29/08/2024\"  # 6. Menambahkan filter untuk tahun 2023\n","        }\n","\n","        # 7. Encode parameter\n","        param = urllib.parse.urlencode(params)\n","\n","        # 8. Melakukan permintaan ke SERP API\n","        r = requests.get('https://serpapi.com/search.json?' + param)\n","        data = r.json()\n","\n","        # 9. Periksa apakah ada hasil berita\n","        if 'news_results' in data:\n","            # 10. Iterasi melalui hasil berita\n","            for item in data['news_results']:\n","                # 11. Menyimpan hasil dalam list yang telah diinisialisasi\n","                url.append(item.get('link', ''))\n","                title.append(item.get('title', ''))\n","                snippet.append(item.get('snippet', ''))  # Menggunakan get() untuk mengambil nilai atau string kosong jika tidak ada\n","                source.append(item.get('source', ''))\n","                date.append(item.get('date', ''))\n","\n","    # 12. Buat DataFrame dari data yang telah diambil\n","    d = {\n","        'no': range(1, len(url) + 1),  # Tambahkan kolom 'no' yang berisi angka berurutan\n","        'url': url,\n","        'title': title,\n","        'content': snippet,\n","        'source': source,\n","        'date': date,\n","    }\n","    df = pd.DataFrame(d)\n","\n","    # 13. Tentukan nama file untuk file Excel\n","    filename = 'Keyword_' + string + '_2024.xlsx'\n","\n","    # 14. Simpan DataFrame ke file Excel\n","    df.to_excel(filename, index=False)  # index=False untuk tidak menyimpan indeks default\n","\n","    # 15. Unduh file Excel\n","    files.download(filename)\n"],"metadata":{"id":"pdi8XeXEd1mh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#preprocessing update\n","# Install library terlebih dahulu\n","!pip install openpyxl\n","!pip install Sastrawi\n","!pip install swifter\n","\n","# Import library yang diperlukan\n","import pandas as pd\n","import re\n","from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import swifter\n","from google.colab import files\n","\n","# Upload file Excel\n","uploaded = files.upload()\n","df = pd.read_excel(next(iter(uploaded.keys())))\n","\n","# Cek apakah kolom 'text' atau 'content' ada di DataFrame\n","if 'Text' not in df.columns and 'content' not in df.columns:\n","    raise ValueError(\"Kolom 'text' atau 'content' tidak ditemukan pada file.\")\n","\n","\n","# Fungsi Preprocessing\n","def case_folding(text):\n","    return text.lower() if isinstance(text, str) else \"\"\n","\n","def filtering(text):\n","    return re.sub(r'[^a-zA-Z\\s]', '', text) if isinstance(text, str) else \"\"\n","\n","def tokenize(text):\n","    return text.split()  # Ganti word_tokenize dengan split biasa\n","\n","def remove_stopwords(tokens):\n","    factory = StopWordRemoverFactory()\n","    stopword_remover = factory.create_stop_word_remover()\n","    filtered_sentence = stopword_remover.remove(' '.join(tokens))\n","    return filtered_sentence.split()\n","\n","def stemming(tokens):\n","    factory = StemmerFactory()\n","    stemmer = factory.create_stemmer()\n","    return [stemmer.stem(word) for word in tokens]\n","\n","def preprocess_text(text):\n","    try:\n","        folded_text = case_folding(text)\n","        filtered_text = filtering(folded_text)\n","        tokens = tokenize(filtered_text)\n","        stop_removed = remove_stopwords(tokens)\n","        stemmed = stemming(stop_removed)\n","        cleaned_text = ' '.join(stemmed)\n","\n","        return {\n","            'Original Text': text,\n","            'Case Folding': folded_text,\n","            'Filtering': filtered_text,\n","            'Token': tokens,\n","            'Stopword': stop_removed,\n","            'Stemming': stemmed,\n","            'Cleaned Text': cleaned_text\n","        }\n","    except Exception as e:\n","        print(f\"Error preprocessing text: {e}\")\n","        return {\n","            'Original Text': text,\n","            'Case Folding': '',\n","            'Filtering': '',\n","            'Token': [],\n","            'Stopword': [],\n","            'Stemming': [],\n","            'Cleaned Text': ''\n","        }\n","\n","# Tentukan kolom teks yang tersedia\n","text_column = 'text' if 'text' in df.columns else 'content'\n","\n","# Terapkan preprocessing\n","preprocessed_df = df[text_column].astype(str).swifter.apply(preprocess_text).apply(pd.Series)\n","\n","# Simpan hasil ke Excel\n","filename = 'hasil_preprocessed_' + list(uploaded.keys())[0]\n","preprocessed_df.to_excel(filename, index=False)\n","files.download(filename)\n","\n","# Tampilkan 5 data pertama\n","preprocessed_df.head()\n"],"metadata":{"id":"-8Izyu5Kd7Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TF-IDF\n","\n","#pake visualisasi\n","import pandas as pd\n","import math\n","from collections import defaultdict\n","from google.colab import files\n","import io\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Unggah file Excel\n","uploaded = files.upload()\n","\n","# Membaca data dari file Excel\n","for filename in uploaded.keys():\n","    df = pd.read_excel(io.BytesIO(uploaded[filename]), sheet_name='Sheet1')\n","\n","# Misalnya teks yang telah di-stemming berada di kolom 'Cleaned Text'\n","documents = df['Cleaned Text'].astype(str).tolist()\n","\n","# Menghitung Term Frequency (TF) untuk setiap dokumen\n","def compute_tf(text):\n","    tf_dict = defaultdict(int)\n","    words = text.split()\n","    for word in words:\n","        tf_dict[word] += 1\n","    total_words = len(words)\n","    for word in tf_dict:\n","        tf_dict[word] = tf_dict[word] / total_words\n","    return tf_dict\n","\n","# Menghitung Document Frequency (DF) untuk semua dokumen\n","def compute_df(docs):\n","    df_dict = defaultdict(int)\n","    for doc in docs:\n","        unique_words = set(doc.split())\n","        for word in unique_words:\n","            df_dict[word] += 1\n","    return df_dict\n","\n","# Menghitung Inverse Document Frequency (IDF) untuk semua dokumen menggunakan logaritma basis 10\n","def compute_idf(total_docs, df_dict):\n","    idf_dict = {}\n","    for word, df in df_dict.items():\n","        idf_dict[word] = math.log10(total_docs / df)\n","    return idf_dict\n","\n","# Menghitung TF-IDF untuk setiap dokumen\n","def compute_tfidf(tf_dict, idf_dict):\n","    tfidf_dict = {}\n","    for word, tf in tf_dict.items():\n","        tfidf_dict[word] = tf * idf_dict.get(word, 0)\n","    return tfidf_dict\n","\n","# Menghitung DF dan IDF secara global\n","total_docs = len(documents)\n","df_dict = compute_df(documents)\n","idf_dict = compute_idf(total_docs, df_dict)\n","\n","# Proses menghitung TF, IDF, dan TF-IDF untuk setiap dokumen\n","tfidf_results = []\n","word_freq = defaultdict(int)  # Dictionary untuk menyimpan frekuensi kata\n","\n","for i, doc in enumerate(documents, start=1):\n","    tf_dict = compute_tf(doc)\n","    tfidf_dict = compute_tfidf(tf_dict, idf_dict)\n","    for word in tf_dict:\n","        tfidf_results.append({\n","            'no': i,\n","            'kata': word,\n","            'TF': tf_dict[word],\n","            'DF': df_dict[word],\n","            'IDF': idf_dict[word],\n","            'TF-IDF': tfidf_dict[word]\n","        })\n","        word_freq[word] += tf_dict[word]  # Menambahkan frekuensi kata ke dictionary\n","\n","# Mengubah hasil TF-IDF menjadi DataFrame\n","tfidf_df = pd.DataFrame(tfidf_results)\n","\n","# Visualisasi 1: WordCloud untuk kata-kata yang paling sering muncul\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n","\n","plt.figure(figsize=(10, 5))\n","plt.title(\"WordCloud dari Kata-Kata yang Paling Sering Muncul\")\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()\n","\n","# Visualisasi 2: Bar plot untuk kata-kata dengan nilai TF-IDF tertinggi\n","top_n = 20  # Jumlah kata dengan nilai TF-IDF tertinggi yang ingin ditampilkan\n","top_tfidf_df = tfidf_df.sort_values(by=\"TF-IDF\", ascending=False).head(top_n)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=\"TF-IDF\", y=\"kata\", data=top_tfidf_df)\n","plt.title(f\"Top {top_n} Kata dengan Nilai TF-IDF Tertinggi\")\n","plt.xlabel(\"Nilai TF-IDF\")\n","plt.ylabel(\"Kata\")\n","plt.show()\n","\n","# Menyimpan hasil ke file Excel baru\n","output_file_path = 'ok fix hasil new tfidf_output.xlsx'\n","tfidf_df.to_excel(output_file_path, index=False)\n","\n","# Mengunduh file Excel\n","files.download(output_file_path)\n","\n","print(\"TF-IDF berhasil dihitung, disimpan di\", output_file_path, \"dan divisualisasikan.\")\n"],"metadata":{"id":"aJSCyrRBd8qE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pelabelan\n","\n","\n","import pandas as pd\n","from google.colab import files\n","import io\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","\n","# Langkah 1: Mengunggah file Excel untuk daftar kata sentimen\n","uploaded_sentiment = files.upload()\n","\n","# Mengambil nama file Excel daftar kata sentimen yang diunggah\n","file_name_sentiment = list(uploaded_sentiment.keys())[0]\n","\n","# Membaca file Excel daftar kata sentimen\n","sentiment_df = pd.read_excel(io.BytesIO(uploaded_sentiment[file_name_sentiment]))\n","\n","# Menampilkan isi DataFrame untuk memastikan file terbaca dengan benar\n","print(\"Daftar Kata Sentimen:\")\n","print(sentiment_df.head())\n","\n","# Langkah 2: Mengunggah file teks untuk dilabeli sentimennya\n","uploaded_text = files.upload()\n","\n","# Mengambil nama file teks yang diunggah\n","file_name_text = list(uploaded_text.keys())[0]\n","\n","# Membaca file teks ke dalam DataFrame dan hanya memilih kolom 'Cleaned Text'\n","df_clean = pd.read_excel(io.BytesIO(uploaded_text[file_name_text]), usecols=['Cleaned Text'])\n","\n","# Menampilkan isi DataFrame untuk memastikan file terbaca dengan benar\n","print(\"\\nTeks yang Akan Dilabeli Sentimen:\")\n","print(df_clean.head())\n","\n","# Mengisi nilai NaN dengan string kosong\n","df_clean = df_clean.fillna('')\n","\n","# Langkah 3: Definisi Fungsi untuk Menghitung Kata Sentimen dan Melabeli Dokumen Berdasarkan Polaritas dan Hate Speech\n","def calculate_polarity_and_hate_speech(text_df, sentiment_df):\n","    polarities = []\n","    hate_speech_counts = []\n","    positive_words = []\n","    negative_words = []\n","    hate_speech_words = []\n","\n","    for index, row in text_df.iterrows():\n","        text = row['Cleaned Text']\n","        if isinstance(text, str):\n","            text = text.lower()\n","        else:\n","            text = ''\n","\n","        polarity = 0\n","        hate_speech_count = 0\n","        pos_words = []\n","        neg_words = []\n","        hate_words = []\n","\n","        for word in text.split():\n","            sentiment_row = sentiment_df[sentiment_df['word'] == word]\n","            if not sentiment_row.empty:\n","                sentiment = sentiment_row['sentiment'].values[0]\n","                if sentiment == 'positive':\n","                    polarity += 1\n","                    pos_words.append(word)\n","                elif sentiment == 'negative':\n","                    polarity -= 1\n","                    neg_words.append(word)\n","                elif sentiment == 'hate_speech':\n","                    hate_speech_count += 1\n","                    hate_words.append(word)\n","\n","        polarities.append(polarity)\n","        hate_speech_counts.append(hate_speech_count)\n","        positive_words.append(\" \".join(pos_words))\n","        negative_words.append(\" \".join(neg_words))\n","        hate_speech_words.append(\" \".join(hate_words))\n","\n","    text_df['polarity'] = polarities\n","    text_df['hate_speech_counts'] = hate_speech_counts\n","    text_df['positive_words'] = positive_words\n","    text_df['negative_words'] = negative_words\n","    text_df['hate_speech_words'] = hate_speech_words\n","    return text_df\n","\n","# Langkah 4: Memanggil Fungsi untuk Menghitung Polaritas dan Hate Speech\n","df_label = calculate_polarity_and_hate_speech(df_clean, sentiment_df)\n","\n","# Langkah 5: Menentukan Label Sentimen dan Menghitung Jumlah Kata pada Masing-masing Kategori\n","for index, row in df_label.iterrows():\n","    polarity = row['polarity']\n","    hate_count = row['hate_speech_counts']\n","\n","    if hate_count > 0:\n","        df_label.loc[index, 'label'] = 'hate_speech'\n","    elif polarity > 0:\n","        df_label.loc[index, 'label'] = 'positive'\n","    elif polarity < 0:\n","        df_label.loc[index, 'label'] = 'negative'\n","    else:\n","        df_label.loc[index, 'label'] = 'neutral'\n","\n","df_label['positive_word_count'] = df_label['positive_words'].apply(lambda x: len(x.split()))\n","df_label['negative_word_count'] = df_label['negative_words'].apply(lambda x: len(x.split()))\n","df_label['hate_speech_word_count'] = df_label['hate_speech_words'].apply(lambda x: len(x.split()))\n","\n","# Menampilkan hasil pelabelan\n","print(\"\\nHasil Pelabelan Sentimen:\")\n","print(df_label.head())\n","\n","# Langkah 5.1: Menghitung Total Jumlah Masing-masing Label\n","label_counts = df_label['label'].value_counts()\n","print(\"\\nTotal Jumlah Masing-masing Label:\")\n","print(label_counts)\n","\n","# Langkah 6: Menyimpan hasil ke file Excel dengan nama file input dari pengguna\n","output_file_name = input(\"Masukkan nama file untuk menyimpan hasil (contoh: 'Pelabelan_Hasil.xlsx'): \")\n","\n","# Jika pengguna tidak menambahkan ekstensi .xlsx, tambahkan secara otomatis\n","if not output_file_name.endswith('.xlsx'):\n","    output_file_name += '.xlsx'\n","\n","df_label.to_excel(output_file_name, index=False)\n","\n","# Mengunduh file Excel yang sudah disimpan\n","files.download(output_file_name)\n","\n","# Langkah 7: Menampilkan Grafik Distribusi Kelas dalam Bentuk Pie Chart\n","# Menghitung jumlah setiap label (sudah dilakukan di Langkah 5.1)\n","# Membuat pie chart\n","plt.figure(figsize=(10, 6))\n","plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])\n","plt.title('Distribusi Kelas Sentimen')\n","plt.axis('equal')  # Memastikan pie chart berbentuk lingkaran\n","plt.show()\n","\n","# Langkah 8: Membuat WordCloud untuk Masing-Masing Kelas\n","# WordCloud untuk Positive\n","positive_words = ' '.join(df_label['positive_words'].tolist())\n","wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_words)\n","\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud_pos, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('WordCloud - Positive Words')\n","plt.show()\n","\n","# WordCloud untuk Negative\n","negative_words = ' '.join(df_label['negative_words'].tolist())\n","wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(negative_words)\n","\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud_neg, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('WordCloud - Negative Words')\n","plt.show()\n","\n","# WordCloud untuk Hate Speech\n","hate_speech_words = ' '.join(df_label['hate_speech_words'].tolist())\n","wordcloud_hate = WordCloud(width=800, height=400, background_color='white').generate(hate_speech_words)\n","\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud_hate, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('WordCloud - Hate Speech Words')\n","plt.show()\n"],"metadata":{"id":"IT7d-6jpe4Gl"},"execution_count":null,"outputs":[]}]}